{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PUMS Microdata Basic Processing\n",
    "This script consumes Public Use Microdata Sample files (PUMS) files to create a merged, normalized file that can be used for analysis and answer specific queries about demographics in Seattle, specifically:\n",
    "\n",
    "1. Number of residents living below Area Median Income (AMI) thresholds\n",
    "2. Racial breakdown of People of Color living at 60% AMI in Seattle\n",
    "3. Languages represented in the neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and filter for Seattle PUMS\n",
    "data_dir = '../data/'\n",
    "\n",
    "# PUMS IDs for each selected PUMA (Seattle: Downtown, Northeast, Northwest, Southeast, and West)\n",
    "SEATTLE_PUMS = [11601, 11602, 11603, 11604, 11605]\n",
    "\n",
    "# Loading household data for Seattle-only locations\n",
    "DF_HOUSEHOLD = pd.read_csv(data_dir + 'ss16hwa.csv')\n",
    "DF_HOUSEHOLD = DF_HOUSEHOLD[DF_HOUSEHOLD['PUMA'].isin(SEATTLE_PUMS)]\n",
    "\n",
    "# Loading person data for Seattle-only locations\n",
    "DF_PERSON = pd.read_csv(data_dir + 'ss16pwa.csv')\n",
    "DF_PERSON = DF_PERSON[DF_PERSON['PUMA'].isin(SEATTLE_PUMS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AMI data tables. The Area median income thresholds are defined by HUD and a function of family size.\n",
    "# Income limits are published here: \n",
    "# https://www.seattle.gov/Documents/Departments/Housing/PropertyManagers/IncomeRentLimits/2018%20Rent%20and%20Income%20Limits.pdf\n",
    "\n",
    "df_AMI = pd.read_csv(data_dir + 'AMI_2016.csv')\n",
    "df_40 = df_AMI[(df_AMI[\"Threshold\"] == \"40% AMI\")]\n",
    "df_50 = df_AMI[(df_AMI[\"Threshold\"] == \"50% AMI\")]\n",
    "df_60 = df_AMI[(df_AMI[\"Threshold\"] == \"60% AMI\")]\n",
    "df_80 = df_AMI[(df_AMI[\"Threshold\"] == \"80% AMI\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Person Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data set for important indices to track\n",
    "DF_PERSON = DF_PERSON[['PUMA', 'SERIALNO', 'RAC1P', 'AGEP', 'DDRS', 'DEAR', 'DEYE',\n",
    "                        'DOUT', 'DPHY', 'ENG', 'JWTR', 'JWRIP', 'LANX', 'PINCP', 'LANP', 'HISP','PWGTP',\n",
    "                      'JWMNP','JWTR','JWRIP','MIG',\"SCHL\"]]\n",
    "\n",
    "# New attributes for the personas\n",
    "# JWMNP = Travel time to work\n",
    "# JWTR = Mode of transport to work\n",
    "# JWRIP = Vehicle occupancy to work\n",
    "# MIG = Mobility status\n",
    "# SCHL = Educational Attainment\n",
    "\n",
    "# Add text columns that correspond with race integers\n",
    "DF_PERSON['race'] = np.where(DF_PERSON['RAC1P'] == 1, \"White alone\", \"\")\n",
    "DF_PERSON['race'] = np.where(DF_PERSON['RAC1P'] == 2, \"Black or African American alone\", DF_PERSON['race'])\n",
    "DF_PERSON['race'] = np.where(DF_PERSON['RAC1P'] == 3, \"American Indian alone\", DF_PERSON['race'])\n",
    "DF_PERSON['race'] = np.where(DF_PERSON['RAC1P'] == 4, \"Alaska Native alone\", DF_PERSON['race'])\n",
    "DF_PERSON['race'] = np.where(DF_PERSON['RAC1P'] == 5, \"American Indian and Alaska Native tribes specified; or American Indian or Alaska Native, not specified and no other races\", DF_PERSON['race'])\n",
    "DF_PERSON['race'] = np.where(DF_PERSON['RAC1P'] == 6, \"Asian alone\", DF_PERSON['race'])\n",
    "DF_PERSON['race'] = np.where(DF_PERSON['RAC1P'] == 7, \"Native Hawaiian and Other Pacific Islander alone\", DF_PERSON['race'])\n",
    "DF_PERSON['race'] = np.where(DF_PERSON['RAC1P'] == 8, \"Some Other Race alone\", DF_PERSON['race'])\n",
    "DF_PERSON['race'] = np.where(DF_PERSON['RAC1P'] == 9, \"Two or More Races\", DF_PERSON['race'])\n",
    "\n",
    "# Add text columns that correspond with race integers\n",
    "DF_PERSON['neighborhood'] = np.where(DF_PERSON['PUMA'] == 11601, \"Northwest\", \"\")\n",
    "DF_PERSON['neighborhood'] = np.where(DF_PERSON['PUMA'] == 11602, \"Northeast\", DF_PERSON['neighborhood'])\n",
    "DF_PERSON['neighborhood'] = np.where(DF_PERSON['PUMA'] == 11603, \"Downtown, Queen Anne, Magnolia\", DF_PERSON['neighborhood'])\n",
    "DF_PERSON['neighborhood'] = np.where(DF_PERSON['PUMA'] == 11604, \"Southeast, Capital Hill\", DF_PERSON['neighborhood'])\n",
    "DF_PERSON['neighborhood'] = np.where(DF_PERSON['PUMA'] == 11605, \"West, Duwamish, Beacon Hill\", DF_PERSON['neighborhood'])\n",
    "\n",
    "# Add text columns that correspond with Hispanic origin/integers\n",
    "DF_PERSON['hispanic'] = np.where(DF_PERSON['HISP'] == 1, \"Not Spanish/Hispanic/Latino origin\", \"\")\n",
    "DF_PERSON['hispanic'] = np.where(DF_PERSON['HISP'] >= 2, \"Spanish/Hispanic/Latino origin\", DF_PERSON['hispanic'])\n",
    "\n",
    "# Language text loaded from Codebook\n",
    "df_codebook = pd.read_csv(data_dir + 'Codebook.csv')\n",
    "df_LANP = df_codebook[(df_codebook[\"Category\"] == \"LANP\")]\n",
    "DF_PERSON = pd.merge(left=DF_PERSON, right=df_LANP, how='left', left_on='LANP', right_on='Code')\n",
    "\n",
    "DF_PERSON.to_csv(data_dir + 'Process_person.csv', mode='w', header=True, index=False)\n",
    "\n",
    "#DF_PERSON.head(-5)\n",
    "#DF_PERSON.count(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Household Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Seattle residents living at-\n",
      "80% AMI: 37%\n",
      "60% AMI: 27%\n",
      "50% AMI: 22%\n",
      "40% AMI: 18%\n"
     ]
    }
   ],
   "source": [
    "# Filter the data set for important indices to track\n",
    "DF_HOUSEHOLD = DF_HOUSEHOLD[['SERIALNO','HINCP','NP','WGTP']]\n",
    "# Filter for income > 1\n",
    "DF_HOUSEHOLD = DF_HOUSEHOLD[(DF_HOUSEHOLD[\"HINCP\"] > 1)]\n",
    "\n",
    "# Escalate income to current year\n",
    "# ref http://www.seattle.gov/financedepartment/cpi/documents/US_CPI_History_--_Annual.pdf\n",
    "DF_HOUSEHOLD[\"HINCP\"]  = DF_HOUSEHOLD[\"HINCP\"] * 245.120 / 234.067\n",
    "                        \n",
    "# Limit very large households to 8 ppl to correspond with AMI tables\n",
    "DF_HOUSEHOLD['NP'] = np.where(DF_HOUSEHOLD[\"NP\"] > 8,8,DF_HOUSEHOLD['NP'])\n",
    "\n",
    "NP_total = DF_HOUSEHOLD['NP'].sum()\n",
    "WGTP_total = DF_HOUSEHOLD['WGTP'].sum()\n",
    "\n",
    "# Add a new index for income below 60% of the median\n",
    "# Add columns for number of people in each AMI threshold\n",
    "DF_HOUSEHOLD = pd.merge(left=DF_HOUSEHOLD, right=df_80, how='left', left_on='NP', right_on='NP')\n",
    "#DF_HOUSEHOLD['AMI_80'] = np.where(DF_HOUSEHOLD['HINCP'] <= DF_HOUSEHOLD['Amount'], DF_HOUSEHOLD['NP'], 0)\n",
    "DF_HOUSEHOLD['AMI_80'] = np.where(DF_HOUSEHOLD['HINCP'] <= DF_HOUSEHOLD['Amount'], DF_HOUSEHOLD['WGTP'], 0)\n",
    "DF_HOUSEHOLD = DF_HOUSEHOLD.drop(['Threshold', 'Amount'], axis=1)\n",
    "\n",
    "DF_HOUSEHOLD = pd.merge(left=DF_HOUSEHOLD, right=df_60, how='left', left_on='NP', right_on='NP')\n",
    "DF_HOUSEHOLD['HINCP_threshold'] = np.where(DF_HOUSEHOLD['HINCP'] <= DF_HOUSEHOLD['Amount'], \"Under 60\", \"Over 60\")\n",
    "#DF_HOUSEHOLD['AMI_60'] = np.where(DF_HOUSEHOLD['HINCP'] <= DF_HOUSEHOLD['Amount'], DF_HOUSEHOLD['NP'], 0)\n",
    "DF_HOUSEHOLD['AMI_60'] = np.where(DF_HOUSEHOLD['HINCP'] <= DF_HOUSEHOLD['Amount'], DF_HOUSEHOLD['WGTP'], 0)\n",
    "DF_HOUSEHOLD = DF_HOUSEHOLD.drop(['Threshold', 'Amount'], axis=1)\n",
    "\n",
    "DF_HOUSEHOLD = pd.merge(left=DF_HOUSEHOLD, right=df_50, how='left', left_on='NP', right_on='NP')\n",
    "#DF_HOUSEHOLD['AMI_50'] = np.where(DF_HOUSEHOLD['HINCP'] <= DF_HOUSEHOLD['Amount'], DF_HOUSEHOLD['NP'], 0)\n",
    "DF_HOUSEHOLD['AMI_50'] = np.where(DF_HOUSEHOLD['HINCP'] <= DF_HOUSEHOLD['Amount'], DF_HOUSEHOLD['WGTP'], 0)\n",
    "DF_HOUSEHOLD = DF_HOUSEHOLD.drop(['Threshold', 'Amount'], axis=1)\n",
    "\n",
    "DF_HOUSEHOLD = pd.merge(left=DF_HOUSEHOLD, right=df_40, how='left', left_on='NP', right_on='NP')\n",
    "#DF_HOUSEHOLD['AMI_40'] = np.where(DF_HOUSEHOLD['HINCP'] <= DF_HOUSEHOLD['Amount'], DF_HOUSEHOLD['NP'], 0)\n",
    "DF_HOUSEHOLD['AMI_40'] = np.where(DF_HOUSEHOLD['HINCP'] <= DF_HOUSEHOLD['Amount'], DF_HOUSEHOLD['WGTP'], 0)\n",
    "DF_HOUSEHOLD = DF_HOUSEHOLD.drop(['Threshold', 'Amount'], axis=1)\n",
    "\n",
    "DF_HOUSEHOLD = pd.merge(left=DF_HOUSEHOLD, right=df_40, how='left', left_on='NP', right_on='NP')\n",
    "#DF_HOUSEHOLD['AMI_40'] = np.where(DF_HOUSEHOLD['HINCP'] <= DF_HOUSEHOLD['Amount'], DF_HOUSEHOLD['NP'], 0)\n",
    "DF_HOUSEHOLD['AMI_40'] = np.where(DF_HOUSEHOLD['HINCP'] <= DF_HOUSEHOLD['Amount'], DF_HOUSEHOLD['WGTP'], 0)\n",
    "DF_HOUSEHOLD = DF_HOUSEHOLD.drop(['Threshold', 'Amount'], axis=1)\n",
    "\n",
    "# Calculate percentages for each household\n",
    "HINCP_UNDER80 = DF_HOUSEHOLD['AMI_80'].sum() / WGTP_total\n",
    "HINCP_UNDER60 = DF_HOUSEHOLD['AMI_60'].sum() / WGTP_total\n",
    "HINCP_UNDER50 = DF_HOUSEHOLD['AMI_50'].sum() / WGTP_total\n",
    "HINCP_UNDER40 = DF_HOUSEHOLD['AMI_40'].sum() / WGTP_total\n",
    "\n",
    "print(\"Number of Seattle residents living at-\")\n",
    "print(\"80% AMI:\", \"{:.0%}\".format(HINCP_UNDER80))\n",
    "print(\"60% AMI:\", \"{:.0%}\".format(HINCP_UNDER60))\n",
    "print(\"50% AMI:\", \"{:.0%}\".format(HINCP_UNDER50))\n",
    "print(\"40% AMI:\", \"{:.0%}\".format(HINCP_UNDER40))\n",
    "\n",
    "DF_HOUSEHOLD.to_csv(data_dir + 'Process_household.csv', mode='w', header=True, index=False)\n",
    "#print (DF_HOUSEHOLD)\n",
    "#print (, HINCP_UNDER60, HINCP_UNDER50, HINCP_UNDER40)\n",
    "#DF_HOUSEHOLD.count(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merged Household and Person Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data set - left join on each Person, so NP should now be set to 1\n",
    "DF_MERGED = pd.DataFrame()\n",
    "DF_MERGED = pd.merge(left=DF_PERSON, right=DF_HOUSEHOLD, how='left', left_on='SERIALNO', right_on='SERIALNO')\n",
    "\n",
    "# Because the merged data is at the HOUSEHOLD level, change weight to WGTP\n",
    "DF_MERGED['AMI_80'] = np.where(DF_MERGED['AMI_80'] > 0, DF_MERGED['WGTP'], 0)\n",
    "DF_MERGED['AMI_60'] = np.where(DF_MERGED['AMI_60'] > 0, DF_MERGED['WGTP'], 0)\n",
    "DF_MERGED['AMI_50'] = np.where(DF_MERGED['AMI_50'] > 0, DF_MERGED['WGTP'], 0)\n",
    "DF_MERGED['AMI_40'] = np.where(DF_MERGED['AMI_40'] > 0, DF_MERGED['WGTP'], 0)\n",
    "\n",
    "DF_MERGED.to_csv(data_dir + 'PUMS_merged.csv', mode='w', header=True, index=False)\n",
    "\n",
    "# Create classification for households under 60% AMI\n",
    "DF_MERGED = DF_MERGED[DF_MERGED[\"HINCP_threshold\"] == \"Under 60\"]\n",
    "\n",
    "# Under 60% aggregated by Hispanic/spanish/latino orgin (of any race), calculate percentages of total, save as csv\n",
    "DF_AGGREGATED_HISPANIC_ORIGIN = pd.DataFrame()\n",
    "#DF_AGGREGATED_HISPANIC_ORIGIN = DF_MERGED.groupby(['hispanic'], as_index=False).agg({'NP':[sum]})\n",
    "DF_AGGREGATED_HISPANIC_ORIGIN = DF_MERGED.groupby(['hispanic'], as_index=False).agg({'WGTP':[sum]})\n",
    "\n",
    "#NP_TOTAL = DF_AGGREGATED_HISPANIC_ORIGIN['NP'].sum()\n",
    "NP_TOTAL = DF_AGGREGATED_HISPANIC_ORIGIN['WGTP'].sum()\n",
    "\n",
    "#DF_AGGREGATED_HISPANIC_ORIGIN[\"pct\"] = DF_AGGREGATED_HISPANIC_ORIGIN[\"NP\"] / NP_TOTAL\n",
    "DF_AGGREGATED_HISPANIC_ORIGIN[\"pct\"] = DF_AGGREGATED_HISPANIC_ORIGIN[\"WGTP\"] / NP_TOTAL\n",
    "\n",
    "DF_AGGREGATED_HISPANIC_ORIGIN.columns = DF_AGGREGATED_HISPANIC_ORIGIN.columns.droplevel(level=1)\n",
    "DF_AGGREGATED_HISPANIC_ORIGIN.to_csv(data_dir + 'AGGREGATED_HISPANIC_ORIGIN.csv', mode='w', header=True, index=False)\n",
    "\n",
    "#DF_MERGED.count(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by Hispanic origin (of any race) and neighborhood, calculate percentage of total, save as csv\n",
    "DF_AGGREGATED_HISPANIC_NEIGHB = pd.DataFrame()\n",
    "#DF_AGGREGATED_RACE_NEIGHB = DF_MERGED.groupby(['neighborhood', 'race'], as_index=False).agg({'NP':[sum]})\n",
    "#DF_AGGREGATED_RACE_NEIGHB.columns = DF_AGGREGATED_RACE_NEIGHB.columns.droplevel(level=1)\n",
    "#NP_TOTAL = DF_AGGREGATED_RACE_NEIGHB['NP'].sum()\n",
    "#DF_AGGREGATED_RACE_NEIGHB[\"pct\"] = DF_AGGREGATED_RACE_NEIGHB[\"NP\"] / NP_TOTAL\n",
    "DF_AGGREGATED_HISPANIC_NEIGHB = DF_MERGED.groupby(['neighborhood', 'hispanic'], as_index=False).agg({'WGTP':[sum]})\n",
    "DF_AGGREGATED_HISPANIC_NEIGHB.columns = DF_AGGREGATED_HISPANIC_NEIGHB.columns.droplevel(level=1)\n",
    "PWGTP_TOTAL =DF_AGGREGATED_HISPANIC_NEIGHB['WGTP'].sum()\n",
    "DF_AGGREGATED_HISPANIC_NEIGHB[\"pct\"] = DF_AGGREGATED_HISPANIC_NEIGHB[\"WGTP\"] / PWGTP_TOTAL\n",
    "DF_AGGREGATED_HISPANIC_NEIGHB.to_csv(data_dir + 'AGGREGATED_HISPANIC_NEIGHB.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Racial breakdown of People of Color living at 60% AMI in Seattle\n",
      "                                                race     WGTP       pct\n",
      "0                              American Indian alone    472.0  0.006140\n",
      "1  American Indian and Alaska Native tribes speci...     68.0  0.000885\n",
      "2                                        Asian alone  30516.0  0.396987\n",
      "3                    Black or African American alone  23354.0  0.303816\n",
      "4   Native Hawaiian and Other Pacific Islander alone   1320.0  0.017172\n",
      "5                              Some Other Race alone   4769.0  0.062041\n",
      "6                                  Two or More Races  16370.0  0.212960\n",
      "Racial breakdown of Spanish/Hispanic/Latino origin (of any race) living at 60% AMI in Seattle\n",
      "                             hispanic      WGTP       pct\n",
      "0  Not Spanish/Hispanic/Latino origin  137904.0  0.923201\n",
      "1      Spanish/Hispanic/Latino origin   11472.0  0.076799\n",
      "Further breakdown: What geographic neighborhoods are represented in this data\n",
      "                      neighborhood  \\\n",
      "0   Downtown, Queen Anne, Magnolia   \n",
      "1   Downtown, Queen Anne, Magnolia   \n",
      "2   Downtown, Queen Anne, Magnolia   \n",
      "3   Downtown, Queen Anne, Magnolia   \n",
      "4   Downtown, Queen Anne, Magnolia   \n",
      "5                        Northeast   \n",
      "6                        Northeast   \n",
      "7                        Northeast   \n",
      "8                        Northeast   \n",
      "9                        Northwest   \n",
      "10                       Northwest   \n",
      "11                       Northwest   \n",
      "12                       Northwest   \n",
      "13                       Northwest   \n",
      "14         Southeast, Capital Hill   \n",
      "15         Southeast, Capital Hill   \n",
      "16         Southeast, Capital Hill   \n",
      "17         Southeast, Capital Hill   \n",
      "18         Southeast, Capital Hill   \n",
      "19     West, Duwamish, Beacon Hill   \n",
      "20     West, Duwamish, Beacon Hill   \n",
      "21     West, Duwamish, Beacon Hill   \n",
      "22     West, Duwamish, Beacon Hill   \n",
      "23     West, Duwamish, Beacon Hill   \n",
      "24     West, Duwamish, Beacon Hill   \n",
      "\n",
      "                                                 race     WGTP  pct  \n",
      "0                               American Indian alone    299.0  NaN  \n",
      "1                                         Asian alone   7418.0  NaN  \n",
      "2                     Black or African American alone   2092.0  NaN  \n",
      "3                               Some Other Race alone    562.0  NaN  \n",
      "4                                   Two or More Races   1757.0  NaN  \n",
      "5                                         Asian alone   7447.0  NaN  \n",
      "6                     Black or African American alone   3334.0  NaN  \n",
      "7                               Some Other Race alone    484.0  NaN  \n",
      "8                                   Two or More Races   1627.0  NaN  \n",
      "9                                         Asian alone   1954.0  NaN  \n",
      "10                    Black or African American alone   1618.0  NaN  \n",
      "11   Native Hawaiian and Other Pacific Islander alone    105.0  NaN  \n",
      "12                              Some Other Race alone    288.0  NaN  \n",
      "13                                  Two or More Races   3473.0  NaN  \n",
      "14                              American Indian alone    131.0  NaN  \n",
      "15                                        Asian alone   3323.0  NaN  \n",
      "16                    Black or African American alone   6689.0  NaN  \n",
      "17   Native Hawaiian and Other Pacific Islander alone   1215.0  NaN  \n",
      "18                                  Two or More Races   4839.0  NaN  \n",
      "19                              American Indian alone     42.0  NaN  \n",
      "20  American Indian and Alaska Native tribes speci...     68.0  NaN  \n",
      "21                                        Asian alone  10374.0  NaN  \n",
      "22                    Black or African American alone   9621.0  NaN  \n",
      "23                              Some Other Race alone   3435.0  NaN  \n",
      "24                                  Two or More Races   4674.0  NaN  \n",
      "Languages represented in the neighborhoods\n",
      "                      neighborhood              Description    WGTP  pct\n",
      "0   Downtown, Queen Anne, Magnolia                  Amharic   385.0  NaN\n",
      "1   Downtown, Queen Anne, Magnolia                  Burmese  1206.0  NaN\n",
      "2   Downtown, Queen Anne, Magnolia                Cantonese   605.0  NaN\n",
      "3   Downtown, Queen Anne, Magnolia                  Chinese    70.0  NaN\n",
      "4   Downtown, Queen Anne, Magnolia                 Filipino   264.0  NaN\n",
      "5   Downtown, Queen Anne, Magnolia                   German    68.0  NaN\n",
      "6   Downtown, Queen Anne, Magnolia                 Mandarin   294.0  NaN\n",
      "7   Downtown, Queen Anne, Magnolia                  Russian    59.0  NaN\n",
      "8   Downtown, Queen Anne, Magnolia                    Shona   146.0  NaN\n",
      "9   Downtown, Queen Anne, Magnolia                  Spanish   439.0  NaN\n",
      "10  Downtown, Queen Anne, Magnolia                  Tagalog   142.0  NaN\n",
      "11  Downtown, Queen Anne, Magnolia               Vietnamese   211.0  NaN\n",
      "12                       Northeast                  Amharic  1283.0  NaN\n",
      "13                       Northeast                   Arabic    97.0  NaN\n",
      "14                       Northeast                  Bengali    99.0  NaN\n",
      "15                       Northeast                Cantonese   549.0  NaN\n",
      "16                       Northeast                  Chinese  2904.0  NaN\n",
      "17                       Northeast                 Japanese   149.0  NaN\n",
      "18                       Northeast                   Korean  1035.0  NaN\n",
      "19                       Northeast                 Mandarin   132.0  NaN\n",
      "20                       Northeast                  Punjabi   136.0  NaN\n",
      "21                       Northeast                  Spanish   287.0  NaN\n",
      "22                       Northeast                     Thai   115.0  NaN\n",
      "23                       Northeast               Vietnamese  1144.0  NaN\n",
      "24                       Northwest                  Amharic   215.0  NaN\n",
      "25                       Northwest                  Chinese   545.0  NaN\n",
      "26                       Northwest                 Japanese   119.0  NaN\n",
      "27                       Northwest                   Korean   155.0  NaN\n",
      "28                       Northwest                      Lao   133.0  NaN\n",
      "29                       Northwest                  Tagalog  2144.0  NaN\n",
      "30         Southeast, Capital Hill                Cantonese   249.0  NaN\n",
      "31         Southeast, Capital Hill                  Chinese   286.0  NaN\n",
      "32         Southeast, Capital Hill                 Japanese   106.0  NaN\n",
      "33         Southeast, Capital Hill                 Mandarin   169.0  NaN\n",
      "34         Southeast, Capital Hill                    Oromo   334.0  NaN\n",
      "35         Southeast, Capital Hill                   Somali   268.0  NaN\n",
      "36         Southeast, Capital Hill                  Swahili   614.0  NaN\n",
      "37         Southeast, Capital Hill                  Tagalog   106.0  NaN\n",
      "38         Southeast, Capital Hill                     Thai   439.0  NaN\n",
      "39         Southeast, Capital Hill               Vietnamese  1438.0  NaN\n",
      "40     West, Duwamish, Beacon Hill                  Amharic   468.0  NaN\n",
      "41     West, Duwamish, Beacon Hill                Cantonese   637.0  NaN\n",
      "42     West, Duwamish, Beacon Hill                  Chinese  2423.0  NaN\n",
      "43     West, Duwamish, Beacon Hill                    Hindi   163.0  NaN\n",
      "44     West, Duwamish, Beacon Hill                  Italian    92.0  NaN\n",
      "45     West, Duwamish, Beacon Hill                 Japanese   110.0  NaN\n",
      "46     West, Duwamish, Beacon Hill                    Khmer   173.0  NaN\n",
      "47     West, Duwamish, Beacon Hill                   Korean   237.0  NaN\n",
      "48     West, Duwamish, Beacon Hill                 Mandarin   539.0  NaN\n",
      "49     West, Duwamish, Beacon Hill                    Oromo  1208.0  NaN\n",
      "50     West, Duwamish, Beacon Hill    Other Mande languages  1026.0  NaN\n",
      "51     West, Duwamish, Beacon Hill  Other languages of Asia   168.0  NaN\n",
      "52     West, Duwamish, Beacon Hill                  Punjabi   122.0  NaN\n",
      "53     West, Duwamish, Beacon Hill                   Somali  3143.0  NaN\n",
      "54     West, Duwamish, Beacon Hill                  Spanish  3060.0  NaN\n",
      "55     West, Duwamish, Beacon Hill                  Tagalog  1815.0  NaN\n",
      "56     West, Duwamish, Beacon Hill                 Tigrinya    75.0  NaN\n",
      "57     West, Duwamish, Beacon Hill               Vietnamese  1833.0  NaN\n"
     ]
    }
   ],
   "source": [
    "# Create classification for households of POC\n",
    "DF_MERGED = DF_MERGED[DF_MERGED[\"race\"] != \"White alone\"]\n",
    "\n",
    "# Under 60% aggregated by race, calculate percentages of total, save as csv\n",
    "DF_AGGREGATED_RACE = pd.DataFrame()\n",
    "#DF_AGGREGATED_RACE = DF_MERGED.groupby(['race'], as_index=False).agg({'NP':[sum]})\n",
    "#NP_TOTAL = DF_AGGREGATED_RACE['NP'].sum()\n",
    "#DF_AGGREGATED_RACE[\"pct\"] = DF_AGGREGATED_RACE[\"NP\"] / NP_TOTAL\n",
    "DF_AGGREGATED_RACE = DF_MERGED.groupby(['race'], as_index=False).agg({'WGTP':[sum]})\n",
    "WGTP_TOTAL = DF_AGGREGATED_RACE['WGTP'].sum()\n",
    "DF_AGGREGATED_RACE[\"pct\"] = DF_AGGREGATED_RACE[\"WGTP\"] / WGTP_TOTAL\n",
    "DF_AGGREGATED_RACE.columns = DF_AGGREGATED_RACE.columns.droplevel(level=1)\n",
    "DF_AGGREGATED_RACE.to_csv(data_dir + 'Aggregated_race.csv', mode='w', header=True, index=False)\n",
    "\n",
    "# Aggregate by race and neighborhood, calculate percentage of total, save as csv\n",
    "DF_AGGREGATED_RACE_NEIGHB = pd.DataFrame()\n",
    "#DF_AGGREGATED_RACE_NEIGHB = DF_MERGED.groupby(['neighborhood', 'race'], as_index=False).agg({'NP':[sum]})\n",
    "#DF_AGGREGATED_RACE_NEIGHB.columns = DF_AGGREGATED_RACE_NEIGHB.columns.droplevel(level=1)\n",
    "#NP_TOTAL = DF_AGGREGATED_RACE_NEIGHB['NP'].sum()\n",
    "#DF_AGGREGATED_RACE_NEIGHB[\"pct\"] = DF_AGGREGATED_RACE_NEIGHB[\"NP\"] / NP_TOTAL\n",
    "DF_AGGREGATED_RACE_NEIGHB = DF_MERGED.groupby(['neighborhood', 'race'], as_index=False).agg({'WGTP':[sum]})\n",
    "DF_AGGREGATED_RACE_NEIGHB.columns = DF_AGGREGATED_RACE_NEIGHB.columns.droplevel(level=1)\n",
    "PWGTP_TOTAL = DF_AGGREGATED_RACE_NEIGHB['WGTP'].sum()\n",
    "DF_AGGREGATED_RACE_NEIGHB[\"pct\"] = DF_AGGREGATED_RACE_NEIGHB[\"WGTP\"] / WGTP_TOTAL\n",
    "DF_AGGREGATED_RACE_NEIGHB.to_csv(data_dir + 'Aggregated_race_neighb.csv', mode='w', header=True, index=False)\n",
    "\n",
    "# Aggregate by language and neighborhood, calulate percentages, save as csv\n",
    "DF_AGGREGATED_LANG_NEIGHB = pd.DataFrame()\n",
    "#DF_AGGREGATED_LANG_NEIGHB = DF_MERGED.groupby(['neighborhood','Description'], as_index=False).agg({'NP':['sum']})\n",
    "#DF_AGGREGATED_LANG_NEIGHB.columns = DF_AGGREGATED_LANG_NEIGHB.columns.droplevel(level=1)\n",
    "#NP_TOTAL = DF_AGGREGATED_LANG_NEIGHB['NP'].sum()\n",
    "DF_AGGREGATED_LANG_NEIGHB = DF_MERGED.groupby(['neighborhood','Description'], as_index=False).agg({'WGTP':['sum']})\n",
    "DF_AGGREGATED_LANG_NEIGHB.columns = DF_AGGREGATED_LANG_NEIGHB.columns.droplevel(level=1)\n",
    "PWGTP_TOTAL = DF_AGGREGATED_LANG_NEIGHB['WGTP'].sum()\n",
    "DF_AGGREGATED_LANG_NEIGHB[\"pct\"] = DF_AGGREGATED_LANG_NEIGHB[\"WGTP\"] / WGTP_TOTAL\n",
    "DF_AGGREGATED_LANG_NEIGHB.to_csv(data_dir + 'Aggregated_lang_neighb.csv', mode='w', header=True, index=False)\n",
    "\n",
    "print(\"Racial breakdown of People of Color living at 60% AMI in Seattle\")\n",
    "print(DF_AGGREGATED_RACE)\n",
    "\n",
    "print(\"Racial breakdown of Spanish/Hispanic/Latino origin (of any race) living at 60% AMI in Seattle\")\n",
    "print(DF_AGGREGATED_HISPANIC_ORIGIN)\n",
    "\n",
    "print(\"Further breakdown: What geographic neighborhoods are represented in this data\")\n",
    "print(DF_AGGREGATED_RACE_NEIGHB)\n",
    "\n",
    "print(\"Languages represented in the neighborhoods\")\n",
    "print(DF_AGGREGATED_LANG_NEIGHB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Data/PUMS_Languages.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-17b88f5233ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mLanguages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mLanguages\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'Other English-based Creole languages'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mwordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLanguages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-17b88f5233ef>\u001b[0m in \u001b[0;36mwordCloud\u001b[1;34m(Text)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"off\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mwordcloud\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./Data/PUMS_Languages.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mLanguages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDF_PERSON\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mto_file\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m         \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    658\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   1930\u001b[0m                 \u001b[1;31m# Open also for reading (\"+\"), because TIFF save_all\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1931\u001b[0m                 \u001b[1;31m# writer needs to go back and edit the written data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1932\u001b[1;33m                 \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w+b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1934\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/PUMS_Languages.png'"
     ]
    }
   ],
   "source": [
    "def wordCloud(Text):\n",
    "    \n",
    "    # Sudo pip install wordcloud\n",
    "    from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from os import path\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    # text wordcloud\n",
    "    # Wordcloud = WordCloud(max_font_size=100, width = 1200, height = 600, mode = 'RGBA', background_color = 'white').generate(' '.join(Text))\n",
    "    wordcloud = WordCloud(collocations=False, max_font_size=100, width =1200, height =600, background_color ='white', colormap=\"ocean\").generate(' '.join(Text))\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    wordcloud.to_file(\"./Data/PUMS_Languages.png\")\n",
    "\n",
    "Languages = DF_PERSON['Description'].tolist()\n",
    "Languages = [x for x in Languages if str(x) != 'nan']\n",
    "Languages = [x for x in Languages if str(x) != 'Nan']\n",
    "Languages = [x for x in Languages if str(x) != 'Other languages of Asia']\n",
    "Languages = [x for x in Languages if str(x) != 'Other Mande languages']\n",
    "Languages = [x for x in Languages if str(x) != 'Other Indo-Iranian languages']\n",
    "Languages = [x for x in Languages if str(x) != 'Other Philippine languages']\n",
    "Languages = [x for x in Languages if str(x) != 'Other and unspecified languages']\n",
    "Languages = [x for x in Languages if str(x) != 'Other Indo-European languages']\n",
    "Languages = [x for x in Languages if str(x) != 'Other Niger-Congo languages']\n",
    "Languages = [x for x in Languages if str(x) != 'Other Central and South American languages']\n",
    "Languages = [x for x in Languages if str(x) != 'Other English-based Creole languages']\n",
    "\n",
    "wordCloud(Languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
